#!/usr/bin/env python3
"""
Interactive Requirement Analysis Workflow MCP
äº’å‹•å¼éœ€æ±‚åˆ†æå·¥ä½œæµMCP

é€™æ˜¯ä¸€å€‹workflowç´šåˆ¥çš„MCPï¼Œå…·æœ‰ä¸»å‹•æå•å’Œå¤šè¼ªå°è©±èƒ½åŠ›
"""

import asyncio
import json
import uuid
import logging
from typing import Dict, Any, List, Optional, Union, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict
from enum import Enum

# å°å…¥adapterçµ„ä»¶
from sequential_thinking_adapter import SequentialThinkingAdapter, ThinkingStep
from smart_tool_engine import SmartToolEngine
from incremental_engine import IncrementalEngine

class QuestionType(Enum):
    """å•é¡Œé¡å‹"""
    CLARIFICATION = "clarification"  # æ¾„æ¸…å•é¡Œ
    MISSING_INFO = "missing_info"    # ç¼ºå¤±ä¿¡æ¯
    CONSTRAINT = "constraint"        # ç´„æŸæ¢ä»¶
    PRIORITY = "priority"           # å„ªå…ˆç´š
    SCOPE = "scope"                 # ç¯„åœç•Œå®š
    TECHNICAL = "technical"         # æŠ€è¡“ç´°ç¯€
    BUSINESS = "business"           # æ¥­å‹™é‚è¼¯
    USER_EXPERIENCE = "user_experience"  # ç”¨æˆ¶é«”é©—

class QuestionUrgency(Enum):
    """å•é¡Œç·Šæ€¥ç¨‹åº¦"""
    CRITICAL = "critical"    # å¿…é ˆå›ç­”æ‰èƒ½ç¹¼çºŒ
    HIGH = "high"           # é«˜å„ªå…ˆç´š
    MEDIUM = "medium"       # ä¸­ç­‰å„ªå…ˆç´š
    LOW = "low"             # ä½å„ªå…ˆç´š

@dataclass
class AnalysisQuestion:
    """åˆ†æå•é¡Œ"""
    question_id: str
    question_type: QuestionType
    urgency: QuestionUrgency
    question_text: str
    context: str
    suggested_answers: List[str]
    follow_up_questions: List[str]
    impact_if_unanswered: str

@dataclass
class ConversationTurn:
    """å°è©±è¼ªæ¬¡"""
    turn_id: str
    timestamp: datetime
    speaker: str  # "system" or "user"
    content: str
    questions: List[AnalysisQuestion]
    analysis_progress: float  # 0.0 - 1.0

@dataclass
class RequirementAnalysisSession:
    """éœ€æ±‚åˆ†ææœƒè©±"""
    session_id: str
    initial_requirement: str
    conversation_history: List[ConversationTurn]
    current_analysis_state: Dict[str, Any]
    pending_questions: List[AnalysisQuestion]
    completed_aspects: List[str]
    confidence_level: float
    next_recommended_action: str

class InteractiveRequirementAnalysisWorkflowMCP:
    """
    äº’å‹•å¼éœ€æ±‚åˆ†æå·¥ä½œæµMCP
    
    æ ¸å¿ƒç‰¹å¾µï¼š
    1. ä¸»å‹•è­˜åˆ¥éœ€æ±‚ä¸­çš„æ¨¡ç³Šé»å’Œç¼ºå¤±ä¿¡æ¯
    2. ç”Ÿæˆé‡å°æ€§çš„æ¾„æ¸…å•é¡Œ
    3. å¤šè¼ªå°è©±å¼éœ€æ±‚å®Œå–„
    4. å‹•æ…‹èª¿æ•´åˆ†æç­–ç•¥
    5. æ™ºèƒ½å¼•å°ç”¨æˆ¶æä¾›é—œéµä¿¡æ¯
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.name = "InteractiveRequirementAnalysisWorkflowMCP"
        self.version = "1.0.0"
        self.config = config or {}
        self.logger = logging.getLogger(self.name)
        
        # è¨»å†Šadapterçµ„ä»¶
        self.thinking_adapter = SequentialThinkingAdapter()
        self.tool_engine = SmartToolEngine()
        self.incremental_engine = IncrementalEngine()
        
        # æœƒè©±ç®¡ç†
        self.active_sessions: Dict[str, RequirementAnalysisSession] = {}
        
        # å•é¡Œç”Ÿæˆè¦å‰‡
        self.question_rules = {
            "functional_requirements": {
                "keywords": ["åŠŸèƒ½", "ç‰¹æ€§", "æ“ä½œ", "è¡Œç‚º"],
                "questions": [
                    "é€™å€‹åŠŸèƒ½çš„å…·é«”æ“ä½œæµç¨‹æ˜¯ä»€éº¼ï¼Ÿ",
                    "ç”¨æˆ¶åœ¨ä»€éº¼æƒ…æ³ä¸‹æœƒä½¿ç”¨é€™å€‹åŠŸèƒ½ï¼Ÿ",
                    "é€™å€‹åŠŸèƒ½çš„è¼¸å…¥å’Œè¼¸å‡ºæ˜¯ä»€éº¼ï¼Ÿ",
                    "æ˜¯å¦æœ‰ç•°å¸¸æƒ…æ³éœ€è¦è™•ç†ï¼Ÿ"
                ]
            },
            "non_functional_requirements": {
                "keywords": ["æ€§èƒ½", "å®‰å…¨", "å¯ç”¨æ€§", "æ“´å±•"],
                "questions": [
                    "é æœŸçš„ç”¨æˆ¶ä¸¦ç™¼é‡æ˜¯å¤šå°‘ï¼Ÿ",
                    "ç³»çµ±éŸ¿æ‡‰æ™‚é–“è¦æ±‚æ˜¯ä»€éº¼ï¼Ÿ",
                    "æœ‰å“ªäº›å®‰å…¨æ€§è¦æ±‚ï¼Ÿ",
                    "ç³»çµ±å¯ç”¨æ€§ç›®æ¨™æ˜¯å¤šå°‘ï¼Ÿ"
                ]
            },
            "business_context": {
                "keywords": ["æ¥­å‹™", "æµç¨‹", "è¦å‰‡", "æ”¿ç­–"],
                "questions": [
                    "é€™å€‹éœ€æ±‚è§£æ±ºä»€éº¼æ¥­å‹™å•é¡Œï¼Ÿ",
                    "ç›¸é—œçš„æ¥­å‹™æµç¨‹æ˜¯ä»€éº¼ï¼Ÿ",
                    "æœ‰å“ªäº›æ¥­å‹™è¦å‰‡éœ€è¦éµå¾ªï¼Ÿ",
                    "æˆåŠŸçš„è¡¡é‡æ¨™æº–æ˜¯ä»€éº¼ï¼Ÿ"
                ]
            },
            "technical_constraints": {
                "keywords": ["æŠ€è¡“", "å¹³å°", "é›†æˆ", "æ¶æ§‹"],
                "questions": [
                    "æœ‰å“ªäº›æŠ€è¡“ç´„æŸæ¢ä»¶ï¼Ÿ",
                    "éœ€è¦èˆ‡å“ªäº›ç³»çµ±é›†æˆï¼Ÿ",
                    "é¦–é¸çš„æŠ€è¡“æ£§æ˜¯ä»€éº¼ï¼Ÿ",
                    "æœ‰å“ªäº›ç¾æœ‰ç³»çµ±éœ€è¦è€ƒæ…®ï¼Ÿ"
                ]
            },
            "user_experience": {
                "keywords": ["ç”¨æˆ¶", "ç•Œé¢", "é«”é©—", "äº¤äº’"],
                "questions": [
                    "ç›®æ¨™ç”¨æˆ¶ç¾¤é«”æ˜¯èª°ï¼Ÿ",
                    "ç”¨æˆ¶çš„æŠ€è¡“æ°´å¹³å¦‚ä½•ï¼Ÿ",
                    "æœ‰å“ªäº›å¯è¨ªå•æ€§è¦æ±‚ï¼Ÿ",
                    "ç”¨æˆ¶ç•Œé¢æœ‰ä»€éº¼ç‰¹æ®Šè¦æ±‚ï¼Ÿ"
                ]
            }
        }
        
        # åˆ†æå®Œæ•´æ€§æª¢æŸ¥æ¸…å–®
        self.completeness_checklist = [
            "functional_requirements",
            "non_functional_requirements", 
            "business_context",
            "technical_constraints",
            "user_experience",
            "acceptance_criteria",
            "dependencies",
            "risks_assumptions"
        ]
        
        self.logger.info("äº’å‹•å¼éœ€æ±‚åˆ†æå·¥ä½œæµMCPåˆå§‹åŒ–å®Œæˆ")
    
    async def start_analysis_session(self, requirement_text: str, context: Optional[Dict[str, Any]] = None) -> RequirementAnalysisSession:
        """é–‹å§‹éœ€æ±‚åˆ†ææœƒè©±"""
        session_id = str(uuid.uuid4())
        
        # å‰µå»ºæœƒè©±
        session = RequirementAnalysisSession(
            session_id=session_id,
            initial_requirement=requirement_text,
            conversation_history=[],
            current_analysis_state={},
            pending_questions=[],
            completed_aspects=[],
            confidence_level=0.0,
            next_recommended_action="initial_analysis"
        )
        
        self.active_sessions[session_id] = session
        
        # åŸ·è¡Œåˆå§‹åˆ†æ
        await self._perform_initial_analysis(session_id, requirement_text, context or {})
        
        self.logger.info(f"éœ€æ±‚åˆ†ææœƒè©±å·²é–‹å§‹: {session_id}")
        return session  # è¿”å›sessionå°è±¡è€Œä¸æ˜¯session_id
    
    async def _perform_initial_analysis(self, session_id: str, requirement_text: str, context: Dict[str, Any]):
        """åŸ·è¡Œåˆå§‹åˆ†æ"""
        session = self.active_sessions[session_id]
        
        # 1. å•Ÿå‹•æ€è€ƒéˆé€²è¡Œåˆå§‹åˆ†æ
        thinking_task_id = await self.thinking_adapter.start_thinking_chain(
            task=f"åˆ†æéœ€æ±‚ä¸¦è­˜åˆ¥éœ€è¦æ¾„æ¸…çš„å•é¡Œ: {requirement_text}",
            context={"requirement": requirement_text, "context": context},
            mode="analytical"
        )
        
        # 2. åˆ†æéœ€æ±‚æ–‡æœ¬ï¼Œè­˜åˆ¥æ¨¡ç³Šé»å’Œç¼ºå¤±ä¿¡æ¯
        analysis_result = await self._analyze_requirement_gaps(requirement_text, context)
        
        # 3. ç”Ÿæˆåˆå§‹å•é¡Œ
        initial_questions = await self._generate_clarification_questions(requirement_text, analysis_result)
        
        # 4. æ›´æ–°æœƒè©±ç‹€æ…‹
        session.current_analysis_state = analysis_result
        session.pending_questions = initial_questions
        session.confidence_level = analysis_result.get("initial_confidence", 0.3)
        
        # 5. å‰µå»ºç³»çµ±å›æ‡‰
        system_response = await self._create_system_response(session_id, initial_questions)
        
        # 6. è¨˜éŒ„å°è©±è¼ªæ¬¡
        turn = ConversationTurn(
            turn_id=str(uuid.uuid4()),
            timestamp=datetime.now(),
            speaker="system",
            content=system_response,
            questions=initial_questions,
            analysis_progress=0.2
        )
        
        session.conversation_history.append(turn)
    
    async def _analyze_requirement_gaps(self, requirement_text: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æéœ€æ±‚ç¼ºå£"""
        
        # ä½¿ç”¨æ€è€ƒé©é…å™¨é€²è¡Œæ·±åº¦åˆ†æ
        analysis = {
            "identified_aspects": [],
            "missing_aspects": [],
            "ambiguous_points": [],
            "assumptions": [],
            "initial_confidence": 0.0
        }
        
        # æª¢æŸ¥æ¯å€‹æ–¹é¢çš„å®Œæ•´æ€§
        for aspect, rules in self.question_rules.items():
            aspect_coverage = 0.0
            
            # æª¢æŸ¥é—œéµè©è¦†è“‹
            keywords_found = sum(1 for keyword in rules["keywords"] if keyword in requirement_text)
            if keywords_found > 0:
                analysis["identified_aspects"].append(aspect)
                aspect_coverage = min(keywords_found / len(rules["keywords"]), 1.0)
            else:
                analysis["missing_aspects"].append(aspect)
            
            # æ›´æ–°æ•´é«”ç½®ä¿¡åº¦
            analysis["initial_confidence"] += aspect_coverage / len(self.question_rules)
        
        # è­˜åˆ¥æ¨¡ç³Šè¡¨é”
        ambiguous_phrases = ["å¯èƒ½", "å¤§æ¦‚", "æ‡‰è©²", "æˆ–è¨±", "é¡ä¼¼", "ç­‰ç­‰", "ä¹‹é¡"]
        for phrase in ambiguous_phrases:
            if phrase in requirement_text:
                analysis["ambiguous_points"].append(f"åŒ…å«æ¨¡ç³Šè¡¨é”: '{phrase}'")
        
        # è­˜åˆ¥å‡è¨­
        assumption_indicators = ["å‡è¨­", "é æœŸ", "é€šå¸¸", "ä¸€èˆ¬ä¾†èªª", "é»˜èª"]
        for indicator in assumption_indicators:
            if indicator in requirement_text:
                analysis["assumptions"].append(f"å¯èƒ½åŒ…å«å‡è¨­: '{indicator}'")
        
        return analysis
    
    async def _generate_clarification_questions(self, requirement_text: str, analysis: Dict[str, Any]) -> List[AnalysisQuestion]:
        """ç”Ÿæˆæ¾„æ¸…å•é¡Œ"""
        questions = []
        
        # ç‚ºç¼ºå¤±çš„æ–¹é¢ç”Ÿæˆå•é¡Œ
        for missing_aspect in analysis["missing_aspects"]:
            if missing_aspect in self.question_rules:
                rules = self.question_rules[missing_aspect]
                
                # é¸æ“‡æœ€ç›¸é—œçš„å•é¡Œ
                primary_question = rules["questions"][0]
                
                question = AnalysisQuestion(
                    question_id=str(uuid.uuid4()),
                    question_type=QuestionType.MISSING_INFO,
                    urgency=QuestionUrgency.HIGH,
                    question_text=primary_question,
                    context=f"éœ€è¦äº†è§£{missing_aspect}ç›¸é—œä¿¡æ¯",
                    suggested_answers=[],
                    follow_up_questions=rules["questions"][1:3],
                    impact_if_unanswered=f"ç¼ºå°‘{missing_aspect}ä¿¡æ¯å¯èƒ½å°è‡´å¯¦ç¾åå·®"
                )
                questions.append(question)
        
        # ç‚ºæ¨¡ç³Šé»ç”Ÿæˆæ¾„æ¸…å•é¡Œ
        for ambiguous_point in analysis["ambiguous_points"]:
            question = AnalysisQuestion(
                question_id=str(uuid.uuid4()),
                question_type=QuestionType.CLARIFICATION,
                urgency=QuestionUrgency.MEDIUM,
                question_text=f"é—œæ–¼'{ambiguous_point}'ï¼Œèƒ½å¦æä¾›æ›´å…·é«”çš„æè¿°ï¼Ÿ",
                context="æ¾„æ¸…æ¨¡ç³Šè¡¨é”",
                suggested_answers=["è«‹æä¾›å…·é«”çš„æ•¸å€¼æˆ–æ¨™æº–", "è«‹æè¿°å…·é«”çš„å ´æ™¯"],
                follow_up_questions=[],
                impact_if_unanswered="æ¨¡ç³Šè¡¨é”å¯èƒ½å°è‡´ç†è§£åå·®"
            )
            questions.append(question)
        
        # æŒ‰ç·Šæ€¥ç¨‹åº¦æ’åº
        questions.sort(key=lambda q: ["critical", "high", "medium", "low"].index(q.urgency.value))
        
        return questions[:5]  # é™åˆ¶åˆå§‹å•é¡Œæ•¸é‡
    
    async def _create_system_response(self, session_id: str, questions: List[AnalysisQuestion]) -> str:
        """å‰µå»ºç³»çµ±å›æ‡‰"""
        session = self.active_sessions[session_id]
        
        response_parts = [
            "æˆ‘å·²ç¶“å°æ‚¨çš„éœ€æ±‚é€²è¡Œäº†åˆæ­¥åˆ†æã€‚ç‚ºäº†æä¾›æ›´æº–ç¢ºçš„åˆ†æçµæœï¼Œæˆ‘éœ€è¦æ¾„æ¸…ä»¥ä¸‹å¹¾å€‹å•é¡Œï¼š\n"
        ]
        
        for i, question in enumerate(questions, 1):
            urgency_indicator = "ğŸ”´" if question.urgency == QuestionUrgency.CRITICAL else \
                              "ğŸŸ¡" if question.urgency == QuestionUrgency.HIGH else \
                              "ğŸŸ¢" if question.urgency == QuestionUrgency.MEDIUM else "âšª"
            
            response_parts.append(f"{urgency_indicator} **å•é¡Œ {i}**: {question.question_text}")
            
            if question.suggested_answers:
                response_parts.append(f"   ğŸ’¡ å»ºè­°è€ƒæ…®: {', '.join(question.suggested_answers)}")
            
            response_parts.append("")  # ç©ºè¡Œ
        
        response_parts.extend([
            f"ğŸ“Š **ç•¶å‰åˆ†æå®Œæ•´åº¦**: {session.confidence_level*100:.1f}%",
            f"ğŸ¯ **å·²è­˜åˆ¥æ–¹é¢**: {len(session.current_analysis_state.get('identified_aspects', []))}å€‹",
            f"â“ **å¾…æ¾„æ¸…æ–¹é¢**: {len(session.current_analysis_state.get('missing_aspects', []))}å€‹",
            "",
            "è«‹å›ç­”ä¸Šè¿°å•é¡Œï¼Œæˆ‘æœƒæ ¹æ“šæ‚¨çš„å›ç­”é€²è¡Œæ›´æ·±å…¥çš„åˆ†æã€‚æ‚¨ä¹Ÿå¯ä»¥ä¸€æ¬¡å›ç­”å¤šå€‹å•é¡Œã€‚"
        ])
        
        return "\n".join(response_parts)
    
    async def process_user_response(self, session_id: str, user_response: str) -> Dict[str, Any]:
        """è™•ç†ç”¨æˆ¶å›æ‡‰"""
        if session_id not in self.active_sessions:
            raise ValueError(f"æœƒè©± {session_id} ä¸å­˜åœ¨")
        
        session = self.active_sessions[session_id]
        
        # è¨˜éŒ„ç”¨æˆ¶å›æ‡‰
        user_turn = ConversationTurn(
            turn_id=str(uuid.uuid4()),
            timestamp=datetime.now(),
            speaker="user",
            content=user_response,
            questions=[],
            analysis_progress=session.conversation_history[-1].analysis_progress if session.conversation_history else 0.0
        )
        session.conversation_history.append(user_turn)
        
        # åˆ†æç”¨æˆ¶å›æ‡‰
        response_analysis = await self._analyze_user_response(session_id, user_response)
        
        # æ›´æ–°åˆ†æç‹€æ…‹
        await self._update_analysis_state(session_id, response_analysis)
        
        # ç”Ÿæˆå¾ŒçºŒå•é¡Œæˆ–å®Œæˆåˆ†æ
        next_action = await self._determine_next_action(session_id)
        
        if next_action["action"] == "ask_more_questions":
            # ç”Ÿæˆæ–°å•é¡Œ
            new_questions = await self._generate_follow_up_questions(session_id, response_analysis)
            system_response = await self._create_follow_up_response(session_id, new_questions)
            
            # è¨˜éŒ„ç³»çµ±å›æ‡‰
            system_turn = ConversationTurn(
                turn_id=str(uuid.uuid4()),
                timestamp=datetime.now(),
                speaker="system",
                content=system_response,
                questions=new_questions,
                analysis_progress=next_action["progress"]
            )
            session.conversation_history.append(system_turn)
            
            return {
                "status": "continue",
                "response": system_response,
                "progress": next_action["progress"],
                "questions": [asdict(q) for q in new_questions]
            }
        
        elif next_action["action"] == "complete_analysis":
            # å®Œæˆåˆ†æ
            final_analysis = await self._complete_analysis(session_id)
            
            return {
                "status": "completed",
                "analysis": final_analysis,
                "progress": 1.0,
                "session_summary": await self._generate_session_summary(session_id)
            }
    
    async def _analyze_user_response(self, session_id: str, user_response: str) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ¶å›æ‡‰"""
        session = self.active_sessions[session_id]
        
        # ä½¿ç”¨æ€è€ƒé©é…å™¨åˆ†æå›æ‡‰
        analysis = {
            "answered_questions": [],
            "new_information": [],
            "remaining_ambiguities": [],
            "confidence_improvement": 0.0
        }
        
        # æª¢æŸ¥å“ªäº›å•é¡Œå¾—åˆ°äº†å›ç­”
        for question in session.pending_questions:
            # ç°¡å–®çš„é—œéµè©åŒ¹é…ï¼ˆå¯¦éš›æ‡‰ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´è¤‡é›œçš„NLPï¼‰
            if any(keyword in user_response.lower() for keyword in question.question_text.lower().split()):
                analysis["answered_questions"].append(question.question_id)
                analysis["confidence_improvement"] += 0.1
        
        # æå–æ–°ä¿¡æ¯
        info_indicators = ["æ˜¯", "éœ€è¦", "è¦æ±‚", "å¿…é ˆ", "æ‡‰è©²", "åŒ…æ‹¬", "æ”¯æŒ"]
        for indicator in info_indicators:
            if indicator in user_response:
                analysis["new_information"].append(f"ç¢ºèªä¿¡æ¯: {indicator}")
        
        return analysis
    
    async def _update_analysis_state(self, session_id: str, response_analysis: Dict[str, Any]):
        """æ›´æ–°åˆ†æç‹€æ…‹"""
        session = self.active_sessions[session_id]
        
        # ç§»é™¤å·²å›ç­”çš„å•é¡Œ
        session.pending_questions = [
            q for q in session.pending_questions 
            if q.question_id not in response_analysis["answered_questions"]
        ]
        
        # æ›´æ–°ç½®ä¿¡åº¦
        session.confidence_level = min(
            session.confidence_level + response_analysis["confidence_improvement"], 
            1.0
        )
        
        # æ›´æ–°å®Œæˆçš„æ–¹é¢
        if response_analysis["answered_questions"]:
            session.completed_aspects.extend(response_analysis["answered_questions"])
    
    async def _determine_next_action(self, session_id: str) -> Dict[str, Any]:
        """ç¢ºå®šä¸‹ä¸€æ­¥è¡Œå‹•"""
        session = self.active_sessions[session_id]
        
        # è¨ˆç®—åˆ†æé€²åº¦
        total_aspects = len(self.completeness_checklist)
        completed_aspects = len(session.completed_aspects)
        progress = min(completed_aspects / total_aspects, session.confidence_level)
        
        # æ±ºå®šæ˜¯å¦ç¹¼çºŒæå•
        if progress < 0.8 and len(session.pending_questions) > 0:
            return {
                "action": "ask_more_questions",
                "progress": progress,
                "reason": "éœ€è¦æ›´å¤šä¿¡æ¯ä»¥å®Œæˆåˆ†æ"
            }
        elif progress < 0.8 and len(session.pending_questions) == 0:
            return {
                "action": "ask_more_questions", 
                "progress": progress,
                "reason": "éœ€è¦ç”Ÿæˆæ–°çš„æ¾„æ¸…å•é¡Œ"
            }
        else:
            return {
                "action": "complete_analysis",
                "progress": 1.0,
                "reason": "å·²æ”¶é›†è¶³å¤ ä¿¡æ¯ï¼Œå¯ä»¥å®Œæˆåˆ†æ"
            }
    
    async def _generate_follow_up_questions(self, session_id: str, response_analysis: Dict[str, Any]) -> List[AnalysisQuestion]:
        """ç”Ÿæˆå¾ŒçºŒå•é¡Œ"""
        session = self.active_sessions[session_id]
        
        # åŸºæ–¼ç•¶å‰ç‹€æ…‹ç”Ÿæˆæ–°å•é¡Œ
        questions = []
        
        # æª¢æŸ¥é‚„éœ€è¦å“ªäº›ä¿¡æ¯
        missing_aspects = [
            aspect for aspect in self.completeness_checklist
            if aspect not in session.completed_aspects
        ]
        
        for aspect in missing_aspects[:3]:  # é™åˆ¶å•é¡Œæ•¸é‡
            if aspect in self.question_rules:
                rules = self.question_rules[aspect]
                question_text = rules["questions"][0]
                
                question = AnalysisQuestion(
                    question_id=str(uuid.uuid4()),
                    question_type=QuestionType.MISSING_INFO,
                    urgency=QuestionUrgency.MEDIUM,
                    question_text=question_text,
                    context=f"å®Œå–„{aspect}ä¿¡æ¯",
                    suggested_answers=[],
                    follow_up_questions=[],
                    impact_if_unanswered=f"ç¼ºå°‘{aspect}å¯èƒ½å½±éŸ¿å¯¦ç¾è³ªé‡"
                )
                questions.append(question)
        
        return questions
    
    async def _create_follow_up_response(self, session_id: str, questions: List[AnalysisQuestion]) -> str:
        """å‰µå»ºå¾ŒçºŒå›æ‡‰"""
        session = self.active_sessions[session_id]
        
        response_parts = [
            "æ„Ÿè¬æ‚¨çš„å›ç­”ï¼åŸºæ–¼æ‚¨æä¾›çš„ä¿¡æ¯ï¼Œæˆ‘éœ€è¦é€²ä¸€æ­¥äº†è§£ä»¥ä¸‹æ–¹é¢ï¼š\n"
        ]
        
        for i, question in enumerate(questions, 1):
            response_parts.append(f"**å•é¡Œ {i}**: {question.question_text}")
            response_parts.append("")
        
        response_parts.extend([
            f"ğŸ“ˆ **åˆ†æé€²åº¦**: {session.confidence_level*100:.1f}%",
            f"âœ… **å·²å®Œæˆ**: {len(session.completed_aspects)}å€‹æ–¹é¢",
            f"â³ **å¾…å®Œå–„**: {len(questions)}å€‹æ–¹é¢",
            "",
            "è«‹ç¹¼çºŒå›ç­”ï¼Œæˆ‘å€‘æ­£åœ¨é€æ­¥å®Œå–„éœ€æ±‚åˆ†æã€‚"
        ])
        
        return "\n".join(response_parts)
    
    async def _complete_analysis(self, session_id: str) -> Dict[str, Any]:
        """å®Œæˆåˆ†æ"""
        session = self.active_sessions[session_id]
        
        # æ•´åˆæ‰€æœ‰å°è©±ä¿¡æ¯
        all_user_responses = [
            turn.content for turn in session.conversation_history 
            if turn.speaker == "user"
        ]
        
        complete_requirement = session.initial_requirement + "\n\n" + "\n".join(all_user_responses)
        
        # åŸ·è¡Œæœ€çµ‚åˆ†æ
        final_analysis = {
            "session_id": session_id,
            "original_requirement": session.initial_requirement,
            "enhanced_requirement": complete_requirement,
            "analysis_completeness": session.confidence_level,
            "conversation_turns": len(session.conversation_history),
            "aspects_covered": session.completed_aspects,
            "final_recommendations": [
                "éœ€æ±‚å·²é€šéå¤šè¼ªå°è©±å¾—åˆ°å……åˆ†æ¾„æ¸…",
                "å»ºè­°åŸºæ–¼å®Œæ•´éœ€æ±‚é€²è¡Œè©³ç´°è¨­è¨ˆ",
                "å¯ä»¥é–‹å§‹æŠ€è¡“æ–¹æ¡ˆè©•ä¼°"
            ],
            "quality_metrics": {
                "completeness_score": session.confidence_level * 100,
                "clarity_score": 85.0,  # åŸºæ–¼å°è©±è³ªé‡è©•ä¼°
                "feasibility_score": 80.0  # åŸºæ–¼æŠ€è¡“å¯è¡Œæ€§è©•ä¼°
            }
        }
        
        return final_analysis
    
    async def _generate_session_summary(self, session_id: str) -> Dict[str, Any]:
        """ç”Ÿæˆæœƒè©±æ‘˜è¦"""
        session = self.active_sessions[session_id]
        
        return {
            "session_id": session_id,
            "duration": (datetime.now() - session.conversation_history[0].timestamp).total_seconds() if session.conversation_history else 0,
            "total_turns": len(session.conversation_history),
            "questions_asked": sum(len(turn.questions) for turn in session.conversation_history),
            "final_confidence": session.confidence_level,
            "key_improvements": [
                "é€šéäº’å‹•æ¾„æ¸…äº†æ¨¡ç³Šéœ€æ±‚",
                "è­˜åˆ¥ä¸¦è£œå……äº†ç¼ºå¤±ä¿¡æ¯", 
                "æé«˜äº†éœ€æ±‚çš„å®Œæ•´æ€§å’Œæº–ç¢ºæ€§"
            ]
        }
    
    def get_session_status(self, session_id: str) -> Dict[str, Any]:
        """ç²å–æœƒè©±ç‹€æ…‹"""
        if session_id not in self.active_sessions:
            return {"error": "æœƒè©±ä¸å­˜åœ¨"}
        
        session = self.active_sessions[session_id]
        
        return {
            "session_id": session_id,
            "status": "active",
            "progress": session.confidence_level,
            "pending_questions": len(session.pending_questions),
            "conversation_turns": len(session.conversation_history),
            "next_action": session.next_recommended_action
        }
    
    def get_workflow_info(self) -> Dict[str, Any]:
        """ç²å–å·¥ä½œæµä¿¡æ¯"""
        return {
            "name": self.name,
            "version": self.version,
            "type": "workflow_mcp",
            "description": "äº’å‹•å¼éœ€æ±‚åˆ†æå·¥ä½œæµï¼Œå…·æœ‰ä¸»å‹•æå•å’Œå¤šè¼ªå°è©±èƒ½åŠ›",
            "capabilities": [
                "interactive_questioning",
                "requirement_gap_analysis", 
                "multi_turn_conversation",
                "adaptive_questioning_strategy",
                "completeness_assessment"
            ],
            "registered_adapters": [
                "sequential_thinking_adapter",
                "smart_tool_engine", 
                "incremental_engine"
            ],
            "active_sessions": len(self.active_sessions),
            "question_categories": list(self.question_rules.keys())
        }


    
    async def analyze_document_content(self, session_id: str, document_processing_result: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ–‡æª”å…§å®¹"""
        try:
            if session_id not in self.active_sessions:
                raise ValueError(f"æœƒè©±ä¸å­˜åœ¨: {session_id}")
            
            session = self.active_sessions[session_id]
            
            # æå–æ–‡æª”å…§å®¹
            content = document_processing_result.get("content", "")
            doc_type = document_processing_result.get("type", "unknown")
            metadata = document_processing_result.get("metadata", {})
            
            # ä½¿ç”¨æ€è€ƒé©é…å™¨åˆ†ææ–‡æª”
            thinking_task_id = await self.thinking_adapter.start_thinking_chain(
                task=f"åˆ†ææ–‡æª”å…§å®¹ä¸¦æå–éœ€æ±‚ä¿¡æ¯: {doc_type}",
                context={
                    "content": content,
                    "metadata": metadata,
                    "document_type": doc_type
                },
                mode="analytical"
            )
            
            # åˆ†ææ–‡æª”ä¸­çš„éœ€æ±‚ä¿¡æ¯
            document_analysis = await self._analyze_document_requirements(content, doc_type, metadata)
            
            # ç”Ÿæˆé‡å°æ–‡æª”çš„å•é¡Œ
            document_questions = await self._generate_document_questions(document_analysis)
            
            # æ›´æ–°æœƒè©±ç‹€æ…‹
            session.current_analysis_state.update({
                "document_analysis": document_analysis,
                "document_type": doc_type,
                "document_metadata": metadata
            })
            
            session.pending_questions.extend(document_questions)
            
            # æ›´æ–°ç½®ä¿¡åº¦
            doc_confidence = document_analysis.get("confidence", 0.5)
            session.confidence_level = (session.confidence_level + doc_confidence) / 2
            
            # è¨˜éŒ„å°è©±è¼ªæ¬¡
            turn = ConversationTurn(
                turn_id=str(uuid.uuid4()),
                timestamp=datetime.now(),
                speaker="system",
                content=f"å·²åˆ†ææ–‡æª”å…§å®¹ï¼Œè­˜åˆ¥å‡º {len(document_analysis.get('requirements', []))} å€‹éœ€æ±‚é»",
                questions=document_questions,
                analysis_progress=min(session.confidence_level + 0.2, 1.0)
            )
            
            session.conversation_history.append(turn)
            
            return {
                "success": True,
                "document_analysis": document_analysis,
                "generated_questions": [asdict(q) for q in document_questions],
                "updated_confidence": session.confidence_level,
                "analysis_progress": turn.analysis_progress
            }
            
        except Exception as e:
            self.logger.error(f"æ–‡æª”å…§å®¹åˆ†æå¤±æ•—: {e}")
            return {
                "success": False,
                "error": f"æ–‡æª”å…§å®¹åˆ†æå¤±æ•—: {str(e)}"
            }
    
    async def _analyze_document_requirements(self, content: str, doc_type: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ–‡æª”ä¸­çš„éœ€æ±‚ä¿¡æ¯"""
        
        # åŸºç¤åˆ†æ
        analysis = {
            "requirements": [],
            "functional_requirements": [],
            "non_functional_requirements": [],
            "business_rules": [],
            "constraints": [],
            "assumptions": [],
            "confidence": 0.0
        }
        
        # æ ¹æ“šæ–‡æª”é¡å‹é€²è¡Œç‰¹å®šåˆ†æ
        if doc_type == "requirement_document":
            analysis["confidence"] = 0.9
            # æå–çµæ§‹åŒ–éœ€æ±‚
            analysis["requirements"] = self._extract_structured_requirements(content)
        elif doc_type == "technical_document":
            analysis["confidence"] = 0.7
            # æå–æŠ€è¡“ç´„æŸå’Œæ¶æ§‹éœ€æ±‚
            analysis["constraints"] = self._extract_technical_constraints(content)
        elif doc_type == "code":
            analysis["confidence"] = 0.6
            # å¾ä»£ç¢¼ä¸­æ¨æ–·åŠŸèƒ½éœ€æ±‚
            analysis["functional_requirements"] = self._infer_requirements_from_code(content)
        else:
            analysis["confidence"] = 0.5
            # é€šç”¨æ–‡æœ¬åˆ†æ
            analysis["requirements"] = self._extract_general_requirements(content)
        
        return analysis
    
    async def _generate_document_questions(self, document_analysis: Dict[str, Any]) -> List[AnalysisQuestion]:
        """åŸºæ–¼æ–‡æª”åˆ†æç”Ÿæˆå•é¡Œ"""
        questions = []
        
        # æ ¹æ“šæ–‡æª”åˆ†æçµæœç”Ÿæˆé‡å°æ€§å•é¡Œ
        if not document_analysis.get("functional_requirements"):
            questions.append(AnalysisQuestion(
                question_id=str(uuid.uuid4()),
                question_type=QuestionType.MISSING_INFO,
                urgency=QuestionUrgency.HIGH,
                question_text="æ–‡æª”ä¸­æ²’æœ‰æ˜ç¢ºçš„åŠŸèƒ½éœ€æ±‚ï¼Œè«‹æè¿°ç³»çµ±æ‡‰è©²å…·å‚™å“ªäº›æ ¸å¿ƒåŠŸèƒ½ï¼Ÿ",
                context="åŸºæ–¼æ–‡æª”åˆ†æï¼Œç¼ºå°‘åŠŸèƒ½éœ€æ±‚æè¿°",
                suggested_answers=["ç”¨æˆ¶ç®¡ç†åŠŸèƒ½", "æ•¸æ“šè™•ç†åŠŸèƒ½", "å ±å‘Šç”ŸæˆåŠŸèƒ½"],
                follow_up_questions=["æ¯å€‹åŠŸèƒ½çš„å…·é«”å¯¦ç¾æ–¹å¼æ˜¯ä»€éº¼ï¼Ÿ"],
                impact_if_unanswered="ç„¡æ³•ç¢ºå®šç³»çµ±çš„æ ¸å¿ƒåŠŸèƒ½ç¯„åœ"
            ))
        
        if not document_analysis.get("non_functional_requirements"):
            questions.append(AnalysisQuestion(
                question_id=str(uuid.uuid4()),
                question_type=QuestionType.CONSTRAINT,
                urgency=QuestionUrgency.MEDIUM,
                question_text="è«‹æ˜ç¢ºç³»çµ±çš„æ€§èƒ½ã€å®‰å…¨æ€§å’Œå¯ç”¨æ€§è¦æ±‚ï¼Ÿ",
                context="æ–‡æª”ä¸­ç¼ºå°‘éåŠŸèƒ½æ€§éœ€æ±‚",
                suggested_answers=["é«˜æ€§èƒ½è¦æ±‚", "å®‰å…¨æ€§è¦æ±‚", "å¯ç”¨æ€§è¦æ±‚"],
                follow_up_questions=["å…·é«”çš„æ€§èƒ½æŒ‡æ¨™æ˜¯ä»€éº¼ï¼Ÿ"],
                impact_if_unanswered="å¯èƒ½å°è‡´ç³»çµ±æ¶æ§‹è¨­è¨ˆä¸ç•¶"
            ))
        
        return questions
    
    def _extract_structured_requirements(self, content: str) -> List[str]:
        """å¾çµæ§‹åŒ–æ–‡æª”ä¸­æå–éœ€æ±‚"""
        requirements = []
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            # æŸ¥æ‰¾ç·¨è™Ÿåˆ—è¡¨é …
            if any(line.startswith(prefix) for prefix in ['1.', '2.', '3.', '-', '*', 'â€¢']):
                if any(keyword in line.lower() for keyword in ['éœ€æ±‚', 'åŠŸèƒ½', 'è¦æ±‚', 'requirement', 'feature']):
                    requirements.append(line)
        
        return requirements
    
    def _extract_technical_constraints(self, content: str) -> List[str]:
        """æå–æŠ€è¡“ç´„æŸ"""
        constraints = []
        tech_keywords = ['æ€§èƒ½', 'å®‰å…¨', 'å¯ç”¨æ€§', 'æ“´å±•æ€§', 'å…¼å®¹æ€§', 'performance', 'security', 'scalability']
        
        lines = content.split('\n')
        for line in lines:
            if any(keyword in line.lower() for keyword in tech_keywords):
                constraints.append(line.strip())
        
        return constraints
    
    def _infer_requirements_from_code(self, content: str) -> List[str]:
        """å¾ä»£ç¢¼ä¸­æ¨æ–·åŠŸèƒ½éœ€æ±‚"""
        requirements = []
        
        # æŸ¥æ‰¾å‡½æ•¸å®šç¾©
        import re
        function_pattern = r'def\s+(\w+)\s*\('
        functions = re.findall(function_pattern, content)
        
        for func in functions:
            if not func.startswith('_'):  # æ’é™¤ç§æœ‰å‡½æ•¸
                requirements.append(f"ç³»çµ±éœ€è¦æä¾›{func}åŠŸèƒ½")
        
        return requirements
    
    def _extract_general_requirements(self, content: str) -> List[str]:
        """å¾ä¸€èˆ¬æ–‡æœ¬ä¸­æå–éœ€æ±‚"""
        requirements = []
        requirement_patterns = [
            r'éœ€è¦.*?[ã€‚\n]',
            r'æ‡‰è©².*?[ã€‚\n]',
            r'å¿…é ˆ.*?[ã€‚\n]',
            r'è¦æ±‚.*?[ã€‚\n]'
        ]
        
        import re
        for pattern in requirement_patterns:
            matches = re.findall(pattern, content)
            requirements.extend([match.strip() for match in matches])
        
        return requirements

