#!/usr/bin/env python3
"""
è‡ªé€²åŒ– MCP ç¥ç¶“ç¶²è·¯ - è‡ªé€²åŒ–å­¸ç¿’å™¨
Self-Evolution Learner for MCP Neural Network

æŒçºŒå­¸ç¿’å’Œå„ªåŒ–ç³»çµ±è¡Œç‚ºï¼Œé©æ‡‰æ–°çš„ä½¿ç”¨æ¨¡å¼
"""

import asyncio
import json
import time
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import defaultdict, deque
import logging

logger = logging.getLogger(__name__)

@dataclass
class LearningPattern:
    """å­¸ç¿’æ¨¡å¼æ•¸æ“šçµæ§‹"""
    pattern_id: str
    pattern_type: str  # 'workflow', 'connection', 'usage', 'performance'
    pattern_data: Dict[str, Any]
    confidence: float
    usage_count: int
    success_rate: float
    discovered_time: datetime
    last_used: datetime

@dataclass
class EvolutionEvent:
    """é€²åŒ–äº‹ä»¶æ•¸æ“šçµæ§‹"""
    event_id: str
    event_type: str  # 'adaptation', 'optimization', 'learning', 'pruning'
    source_component: str
    target_component: Optional[str]
    event_data: Dict[str, Any]
    impact_score: float
    timestamp: datetime

class SelfEvolutionLearner:
    """
    è‡ªé€²åŒ–å­¸ç¿’å™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æŒçºŒå­¸ç¿’ç³»çµ±ä½¿ç”¨æ¨¡å¼
    2. è­˜åˆ¥å’Œå¼·åŒ–æˆåŠŸæ¨¡å¼
    3. è‡ªå‹•é©æ‡‰æ–°çš„éœ€æ±‚
    4. å„ªåŒ–æ•´é«”ç³»çµ±æ€§èƒ½
    """
    
    def __init__(self):
        self.learned_patterns: Dict[str, LearningPattern] = {}
        self.evolution_events: List[EvolutionEvent] = []
        self.behavior_history: deque = deque(maxlen=1000)
        
        # å­¸ç¿’åƒæ•¸
        self.learning_threshold = 0.7
        self.pattern_confidence_threshold = 0.8
        self.adaptation_sensitivity = 0.1
        self.evolution_cycle_interval = 1800  # 30åˆ†é˜
        
        # æ¨¡å¼è­˜åˆ¥åƒæ•¸
        self.min_pattern_occurrences = 5
        self.pattern_similarity_threshold = 0.85
        self.pattern_decay_rate = 0.05
        
        # è‡ªé©æ‡‰åƒæ•¸
        self.performance_window = 100  # æœ€è¿‘100æ¬¡äº¤äº’
        self.adaptation_history: deque = deque(maxlen=50)
        
        # é€²åŒ–ç‹€æ…‹
        self.is_learning = False
        self.last_evolution_cycle = datetime.now()
        self.evolution_generation = 0
        
    async def start_learning(self):
        """å•Ÿå‹•æŒçºŒå­¸ç¿’éç¨‹"""
        self.is_learning = True
        logger.info("ğŸ§  è‡ªé€²åŒ–å­¸ç¿’å™¨å•Ÿå‹•")
        
        while self.is_learning:
            try:
                await self.evolution_cycle()
                await asyncio.sleep(self.evolution_cycle_interval)
            except Exception as e:
                logger.error(f"é€²åŒ–å¾ªç’°éŒ¯èª¤: {e}")
                await asyncio.sleep(60)
    
    async def evolution_cycle(self):
        """å–®æ¬¡é€²åŒ–å¾ªç’°"""
        self.evolution_generation += 1
        logger.info(f"ğŸ”„ é–‹å§‹é€²åŒ–å¾ªç’° #{self.evolution_generation}")
        
        # 1. åˆ†æè¡Œç‚ºæ¨¡å¼
        await self.analyze_behavior_patterns()
        
        # 2. è­˜åˆ¥æ–°çš„å­¸ç¿’æ¨¡å¼
        await self.identify_learning_patterns()
        
        # 3. å„ªåŒ–ç¾æœ‰æ¨¡å¼
        await self.optimize_existing_patterns()
        
        # 4. é©æ‡‰æ€§èª¿æ•´
        await self.adaptive_adjustment()
        
        # 5. æ¸…ç†éæ™‚æ¨¡å¼
        await self.prune_obsolete_patterns()
        
        # 6. è¨˜éŒ„é€²åŒ–äº‹ä»¶
        self.record_evolution_event(
            event_type='evolution_cycle',
            source_component='self_evolution_learner',
            event_data={
                'generation': self.evolution_generation,
                'patterns_count': len(self.learned_patterns),
                'events_count': len(self.evolution_events)
            },
            impact_score=0.5
        )
        
        self.last_evolution_cycle = datetime.now()
        logger.info(f"âœ… é€²åŒ–å¾ªç’° #{self.evolution_generation} å®Œæˆ")
    
    def record_behavior(self, behavior_data: Dict[str, Any]):
        """è¨˜éŒ„ç³»çµ±è¡Œç‚ºæ•¸æ“š"""
        behavior_entry = {
            'timestamp': datetime.now(),
            'data': behavior_data
        }
        
        self.behavior_history.append(behavior_entry)
        logger.debug(f"ğŸ“ è¨˜éŒ„è¡Œç‚º: {behavior_data.get('action', 'unknown')}")
    
    async def analyze_behavior_patterns(self):
        """åˆ†æè¡Œç‚ºæ¨¡å¼"""
        if len(self.behavior_history) < self.min_pattern_occurrences:
            return
        
        logger.info("ğŸ” åˆ†æè¡Œç‚ºæ¨¡å¼")
        
        # åˆ†æå·¥ä½œæµæ¨¡å¼
        workflow_patterns = self.extract_workflow_patterns()
        
        # åˆ†æé€£æ¥æ¨¡å¼
        connection_patterns = self.extract_connection_patterns()
        
        # åˆ†æä½¿ç”¨æ¨¡å¼
        usage_patterns = self.extract_usage_patterns()
        
        # åˆ†ææ€§èƒ½æ¨¡å¼
        performance_patterns = self.extract_performance_patterns()
        
        logger.info(f"ğŸ“Š ç™¼ç¾æ¨¡å¼: å·¥ä½œæµ={len(workflow_patterns)}, é€£æ¥={len(connection_patterns)}, ä½¿ç”¨={len(usage_patterns)}, æ€§èƒ½={len(performance_patterns)}")
    
    def extract_workflow_patterns(self) -> List[Dict]:
        """æå–å·¥ä½œæµæ¨¡å¼"""
        workflow_sequences = defaultdict(int)
        
        # åˆ†ææœ€è¿‘çš„è¡Œç‚ºåºåˆ—
        recent_behaviors = list(self.behavior_history)[-50:]
        
        for i in range(len(recent_behaviors) - 2):
            sequence = []
            for j in range(3):  # 3æ­¥åºåˆ—
                if i + j < len(recent_behaviors):
                    behavior = recent_behaviors[i + j]['data']
                    if 'workflow' in behavior:
                        sequence.append(behavior['workflow'])
            
            if len(sequence) == 3:
                sequence_key = ' -> '.join(sequence)
                workflow_sequences[sequence_key] += 1
        
        # è­˜åˆ¥é »ç¹æ¨¡å¼
        patterns = []
        for sequence, count in workflow_sequences.items():
            if count >= self.min_pattern_occurrences:
                patterns.append({
                    'sequence': sequence,
                    'frequency': count,
                    'confidence': min(1.0, count / 10.0)
                })
        
        return patterns
    
    def extract_connection_patterns(self) -> List[Dict]:
        """æå–é€£æ¥æ¨¡å¼"""
        connection_pairs = defaultdict(int)
        
        for behavior in self.behavior_history:
            data = behavior['data']
            if 'source' in data and 'target' in data:
                pair = f"{data['source']} -> {data['target']}"
                connection_pairs[pair] += 1
        
        patterns = []
        for pair, count in connection_pairs.items():
            if count >= self.min_pattern_occurrences:
                patterns.append({
                    'connection': pair,
                    'frequency': count,
                    'confidence': min(1.0, count / 20.0)
                })
        
        return patterns
    
    def extract_usage_patterns(self) -> List[Dict]:
        """æå–ä½¿ç”¨æ¨¡å¼"""
        hourly_usage = defaultdict(int)
        component_usage = defaultdict(int)
        
        for behavior in self.behavior_history:
            timestamp = behavior['timestamp']
            hour = timestamp.hour
            hourly_usage[hour] += 1
            
            data = behavior['data']
            if 'component' in data:
                component_usage[data['component']] += 1
        
        patterns = []
        
        # æ™‚é–“æ¨¡å¼
        peak_hours = sorted(hourly_usage.items(), key=lambda x: x[1], reverse=True)[:3]
        if peak_hours:
            patterns.append({
                'type': 'time_pattern',
                'peak_hours': [hour for hour, count in peak_hours],
                'confidence': 0.8
            })
        
        # çµ„ä»¶ä½¿ç”¨æ¨¡å¼
        popular_components = sorted(component_usage.items(), key=lambda x: x[1], reverse=True)[:5]
        if popular_components:
            patterns.append({
                'type': 'component_usage',
                'popular_components': [comp for comp, count in popular_components],
                'confidence': 0.7
            })
        
        return patterns
    
    def extract_performance_patterns(self) -> List[Dict]:
        """æå–æ€§èƒ½æ¨¡å¼"""
        response_times = []
        success_rates = defaultdict(list)
        
        for behavior in self.behavior_history:
            data = behavior['data']
            
            if 'response_time' in data:
                response_times.append(data['response_time'])
            
            if 'component' in data and 'success' in data:
                success_rates[data['component']].append(data['success'])
        
        patterns = []
        
        # éŸ¿æ‡‰æ™‚é–“æ¨¡å¼
        if response_times:
            avg_response_time = np.mean(response_times)
            patterns.append({
                'type': 'response_time',
                'avg_time': avg_response_time,
                'trend': 'improving' if len(response_times) > 10 and np.mean(response_times[-10:]) < avg_response_time else 'stable',
                'confidence': 0.6
            })
        
        # æˆåŠŸç‡æ¨¡å¼
        for component, successes in success_rates.items():
            if len(successes) >= 5:
                success_rate = sum(successes) / len(successes)
                patterns.append({
                    'type': 'success_rate',
                    'component': component,
                    'rate': success_rate,
                    'confidence': min(1.0, len(successes) / 20.0)
                })
        
        return patterns
    
    async def identify_learning_patterns(self):
        """è­˜åˆ¥æ–°çš„å­¸ç¿’æ¨¡å¼"""
        logger.info("ğŸ¯ è­˜åˆ¥å­¸ç¿’æ¨¡å¼")
        
        # é€™è£¡å¯ä»¥å¯¦ç¾æ›´è¤‡é›œçš„æ¨¡å¼è­˜åˆ¥ç®—æ³•
        # ä¾‹å¦‚ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æŠ€è¡“ä¾†è­˜åˆ¥éš±è—æ¨¡å¼
        
        # ç°¡åŒ–ç‰ˆæœ¬ï¼šåŸºæ–¼é »ç‡å’ŒæˆåŠŸç‡è­˜åˆ¥æ¨¡å¼
        pattern_candidates = self.generate_pattern_candidates()
        
        for candidate in pattern_candidates:
            pattern_id = self.generate_pattern_id(candidate)
            
            if pattern_id not in self.learned_patterns:
                # å‰µå»ºæ–°çš„å­¸ç¿’æ¨¡å¼
                pattern = LearningPattern(
                    pattern_id=pattern_id,
                    pattern_type=candidate['type'],
                    pattern_data=candidate['data'],
                    confidence=candidate['confidence'],
                    usage_count=1,
                    success_rate=candidate.get('success_rate', 0.5),
                    discovered_time=datetime.now(),
                    last_used=datetime.now()
                )
                
                self.learned_patterns[pattern_id] = pattern
                logger.info(f"ğŸ†• ç™¼ç¾æ–°æ¨¡å¼: {pattern_id}")
                
                # è¨˜éŒ„é€²åŒ–äº‹ä»¶
                self.record_evolution_event(
                    event_type='learning',
                    source_component='pattern_identifier',
                    event_data={'pattern_id': pattern_id, 'pattern_type': candidate['type']},
                    impact_score=candidate['confidence']
                )
    
    def generate_pattern_candidates(self) -> List[Dict]:
        """ç”Ÿæˆæ¨¡å¼å€™é¸"""
        candidates = []
        
        # åŸºæ–¼æœ€è¿‘è¡Œç‚ºç”Ÿæˆå€™é¸æ¨¡å¼
        recent_behaviors = list(self.behavior_history)[-20:]
        
        # å·¥ä½œæµåºåˆ—å€™é¸
        for i in range(len(recent_behaviors) - 1):
            current = recent_behaviors[i]['data']
            next_behavior = recent_behaviors[i + 1]['data']
            
            if 'workflow' in current and 'workflow' in next_behavior:
                candidates.append({
                    'type': 'workflow_sequence',
                    'data': {
                        'from': current['workflow'],
                        'to': next_behavior['workflow']
                    },
                    'confidence': 0.6,
                    'success_rate': 0.7
                })
        
        return candidates
    
    def generate_pattern_id(self, candidate: Dict) -> str:
        """ç”Ÿæˆæ¨¡å¼ID"""
        pattern_type = candidate['type']
        data_str = json.dumps(candidate['data'], sort_keys=True)
        return f"{pattern_type}_{hash(data_str) % 10000:04d}"
    
    async def optimize_existing_patterns(self):
        """å„ªåŒ–ç¾æœ‰æ¨¡å¼"""
        logger.info("âš¡ å„ªåŒ–ç¾æœ‰æ¨¡å¼")
        
        for pattern_id, pattern in self.learned_patterns.items():
            # æ›´æ–°æ¨¡å¼ä¿¡å¿ƒåº¦
            self.update_pattern_confidence(pattern)
            
            # æ‡‰ç”¨æ¨¡å¼è¡°æ¸›
            self.apply_pattern_decay(pattern)
            
            # æª¢æŸ¥æ¨¡å¼æœ‰æ•ˆæ€§
            if pattern.confidence < self.pattern_confidence_threshold * 0.5:
                logger.info(f"ğŸ—‘ï¸ æ¨™è¨˜éæ™‚æ¨¡å¼: {pattern_id}")
                pattern.confidence = 0.0  # æ¨™è¨˜ç‚ºå¾…åˆªé™¤
    
    def update_pattern_confidence(self, pattern: LearningPattern):
        """æ›´æ–°æ¨¡å¼ä¿¡å¿ƒåº¦"""
        # åŸºæ–¼ä½¿ç”¨é »ç‡å’ŒæˆåŠŸç‡æ›´æ–°ä¿¡å¿ƒåº¦
        usage_factor = min(1.0, pattern.usage_count / 50.0)
        success_factor = pattern.success_rate
        time_factor = max(0.1, 1.0 - (datetime.now() - pattern.last_used).days / 30.0)
        
        new_confidence = (usage_factor + success_factor + time_factor) / 3.0
        pattern.confidence = new_confidence
    
    def apply_pattern_decay(self, pattern: LearningPattern):
        """æ‡‰ç”¨æ¨¡å¼è¡°æ¸›"""
        days_since_use = (datetime.now() - pattern.last_used).days
        decay_factor = max(0.1, 1.0 - (days_since_use * self.pattern_decay_rate))
        pattern.confidence *= decay_factor
    
    async def adaptive_adjustment(self):
        """é©æ‡‰æ€§èª¿æ•´"""
        logger.info("ğŸ”§ é©æ‡‰æ€§èª¿æ•´")
        
        # åˆ†æç³»çµ±æ€§èƒ½è¶¨å‹¢
        performance_trend = self.analyze_performance_trend()
        
        # æ ¹æ“šè¶¨å‹¢èª¿æ•´åƒæ•¸
        if performance_trend == 'declining':
            # æ€§èƒ½ä¸‹é™ï¼Œå¢åŠ å­¸ç¿’æ•æ„Ÿåº¦
            self.adaptation_sensitivity = min(0.3, self.adaptation_sensitivity * 1.2)
            logger.info("ğŸ“ˆ å¢åŠ å­¸ç¿’æ•æ„Ÿåº¦")
        elif performance_trend == 'improving':
            # æ€§èƒ½æ”¹å–„ï¼Œå¯ä»¥é™ä½æ•æ„Ÿåº¦
            self.adaptation_sensitivity = max(0.05, self.adaptation_sensitivity * 0.9)
            logger.info("ğŸ“‰ é™ä½å­¸ç¿’æ•æ„Ÿåº¦")
        
        # è¨˜éŒ„é©æ‡‰æ€§èª¿æ•´
        self.adaptation_history.append({
            'timestamp': datetime.now(),
            'trend': performance_trend,
            'sensitivity': self.adaptation_sensitivity
        })
    
    def analyze_performance_trend(self) -> str:
        """åˆ†ææ€§èƒ½è¶¨å‹¢"""
        if len(self.behavior_history) < 20:
            return 'stable'
        
        recent_behaviors = list(self.behavior_history)[-20:]
        older_behaviors = list(self.behavior_history)[-40:-20] if len(self.behavior_history) >= 40 else []
        
        if not older_behaviors:
            return 'stable'
        
        # è¨ˆç®—å¹³å‡éŸ¿æ‡‰æ™‚é–“
        recent_times = [b['data'].get('response_time', 1.0) for b in recent_behaviors if 'response_time' in b['data']]
        older_times = [b['data'].get('response_time', 1.0) for b in older_behaviors if 'response_time' in b['data']]
        
        if not recent_times or not older_times:
            return 'stable'
        
        recent_avg = np.mean(recent_times)
        older_avg = np.mean(older_times)
        
        if recent_avg < older_avg * 0.9:
            return 'improving'
        elif recent_avg > older_avg * 1.1:
            return 'declining'
        else:
            return 'stable'
    
    async def prune_obsolete_patterns(self):
        """æ¸…ç†éæ™‚æ¨¡å¼"""
        obsolete_patterns = [
            pattern_id for pattern_id, pattern in self.learned_patterns.items()
            if pattern.confidence <= 0.0
        ]
        
        for pattern_id in obsolete_patterns:
            del self.learned_patterns[pattern_id]
            logger.info(f"ğŸ—‘ï¸ ç§»é™¤éæ™‚æ¨¡å¼: {pattern_id}")
            
            # è¨˜éŒ„æ¸…ç†äº‹ä»¶
            self.record_evolution_event(
                event_type='pruning',
                source_component='pattern_pruner',
                event_data={'pattern_id': pattern_id},
                impact_score=0.2
            )
    
    def record_evolution_event(self, event_type: str, source_component: str, 
                             event_data: Dict[str, Any], impact_score: float,
                             target_component: Optional[str] = None):
        """è¨˜éŒ„é€²åŒ–äº‹ä»¶"""
        event = EvolutionEvent(
            event_id=f"{event_type}_{int(time.time())}_{len(self.evolution_events)}",
            event_type=event_type,
            source_component=source_component,
            target_component=target_component,
            event_data=event_data,
            impact_score=impact_score,
            timestamp=datetime.now()
        )
        
        self.evolution_events.append(event)
        
        # ä¿æŒäº‹ä»¶æ­·å²åœ¨åˆç†ç¯„åœå…§
        if len(self.evolution_events) > 1000:
            self.evolution_events = self.evolution_events[-500:]
    
    def get_learning_stats(self) -> Dict:
        """ç²å–å­¸ç¿’çµ±è¨ˆä¿¡æ¯"""
        active_patterns = [p for p in self.learned_patterns.values() if p.confidence > 0.1]
        
        return {
            'evolution_generation': self.evolution_generation,
            'total_patterns': len(self.learned_patterns),
            'active_patterns': len(active_patterns),
            'behavior_history_size': len(self.behavior_history),
            'evolution_events': len(self.evolution_events),
            'adaptation_sensitivity': self.adaptation_sensitivity,
            'last_evolution_cycle': self.last_evolution_cycle,
            'avg_pattern_confidence': np.mean([p.confidence for p in active_patterns]) if active_patterns else 0.0
        }
    
    def get_top_patterns(self, limit: int = 10) -> List[LearningPattern]:
        """ç²å–æœ€ä½³å­¸ç¿’æ¨¡å¼"""
        active_patterns = [p for p in self.learned_patterns.values() if p.confidence > 0.1]
        active_patterns.sort(key=lambda x: x.confidence * x.success_rate, reverse=True)
        return active_patterns[:limit]
    
    def stop_learning(self):
        """åœæ­¢å­¸ç¿’éç¨‹"""
        self.is_learning = False
        logger.info("ğŸ›‘ è‡ªé€²åŒ–å­¸ç¿’å™¨åœæ­¢")

# æ¸¬è©¦å’Œæ¼”ç¤ºä»£ç¢¼
def test_evolution_learner():
    """æ¸¬è©¦è‡ªé€²åŒ–å­¸ç¿’å™¨"""
    learner = SelfEvolutionLearner()
    
    print("ğŸ§ª æ¸¬è©¦è‡ªé€²åŒ–å­¸ç¿’å™¨")
    
    # æ¨¡æ“¬ä¸€äº›è¡Œç‚ºæ•¸æ“š
    behaviors = [
        {'action': 'workflow_start', 'workflow': 'smartui_development', 'component': 'product_orchestrator', 'response_time': 0.5, 'success': True},
        {'action': 'mcp_call', 'source': 'product_orchestrator', 'target': 'smartui_mcp', 'response_time': 1.2, 'success': True},
        {'action': 'mcp_call', 'source': 'smartui_mcp', 'target': 'kilocode_mcp', 'response_time': 2.1, 'success': True},
        {'action': 'workflow_complete', 'workflow': 'smartui_development', 'component': 'product_orchestrator', 'response_time': 0.3, 'success': True},
    ]
    
    # è¨˜éŒ„å¤šæ¬¡è¡Œç‚º
    for _ in range(10):
        for behavior in behaviors:
            learner.record_behavior(behavior)
    
    # åŸ·è¡Œä¸€æ¬¡é€²åŒ–å¾ªç’°
    import asyncio
    asyncio.run(learner.evolution_cycle())
    
    # é¡¯ç¤ºå­¸ç¿’çµ±è¨ˆ
    stats = learner.get_learning_stats()
    print("\nğŸ“Š å­¸ç¿’çµ±è¨ˆ:")
    print(json.dumps(stats, indent=2, default=str))
    
    # é¡¯ç¤ºæœ€ä½³æ¨¡å¼
    top_patterns = learner.get_top_patterns(5)
    print(f"\nğŸ† æœ€ä½³å­¸ç¿’æ¨¡å¼ ({len(top_patterns)}):")
    for pattern in top_patterns:
        print(f"  {pattern.pattern_id}: ä¿¡å¿ƒåº¦={pattern.confidence:.3f}, æˆåŠŸç‡={pattern.success_rate:.3f}")

if __name__ == "__main__":
    test_evolution_learner()

