#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Workflow MCP CLI
å¢å¼ºå‹å·¥ä½œæµMCPå‘½ä»¤è¡Œæ¥å£

æä¾›å¢å¼ºå‹å·¥ä½œæµå¼•æ“çš„å‘½ä»¤è¡Œæ“ä½œæ¥å£
"""

import asyncio
import json
import sys
import argparse
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from enhanced_workflow_engine import EnhancedWorkflowEngine
from dynamic_workflow_generator import DynamicWorkflowGenerator
from parallel_execution_scheduler import ParallelExecutionScheduler
from intelligent_dependency_manager import IntelligentDependencyManager
from workflow_state_manager import WorkflowStateManager

class EnhancedWorkflowCLI:
    """å¢å¼ºå‹å·¥ä½œæµCLI"""
    
    def __init__(self):
        self.engine = EnhancedWorkflowEngine()
        self.generator = DynamicWorkflowGenerator()
        self.scheduler = ParallelExecutionScheduler()
        self.dependency_manager = IntelligentDependencyManager()
        self.state_manager = WorkflowStateManager()
    
    async def create_workflow(self, args):
        """åˆ›å»ºå·¥ä½œæµ"""
        try:
            workflow_config = {
                "name": args.name,
                "description": args.description or f"å·¥ä½œæµ: {args.name}",
                "template_type": args.template or "basic",
                "requirements": args.requirements.split(",") if args.requirements else [],
                "parallel_enabled": args.parallel,
                "optimization_enabled": args.optimize
            }
            
            result = await self.generator.generate_workflow(workflow_config)
            
            if result["status"] == "success":
                print(f"âœ… å·¥ä½œæµåˆ›å»ºæˆåŠŸ: {result['workflow_id']}")
                print(f"ğŸ“‹ èŠ‚ç‚¹æ•°é‡: {result['nodes_count']}")
                print(f"ğŸ”— è¾¹æ•°é‡: {result['edges_count']}")
                
                if args.output:
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    print(f"ğŸ’¾ å·¥ä½œæµå·²ä¿å­˜åˆ°: {args.output}")
            else:
                print(f"âŒ å·¥ä½œæµåˆ›å»ºå¤±è´¥: {result['message']}")
                return 1
                
        except Exception as e:
            print(f"âŒ åˆ›å»ºå·¥ä½œæµæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return 1
        
        return 0
    
    async def execute_workflow(self, args):
        """æ‰§è¡Œå·¥ä½œæµ"""
        try:
            # åŠ è½½å·¥ä½œæµ
            if args.file:
                with open(args.file, 'r', encoding='utf-8') as f:
                    workflow_data = json.load(f)
                workflow_id = workflow_data.get("workflow_id")
            else:
                workflow_id = args.id
            
            if not workflow_id:
                print("âŒ è¯·æä¾›å·¥ä½œæµIDæˆ–æ–‡ä»¶")
                return 1
            
            # æ‰§è¡Œå‚æ•°
            execution_config = {
                "workflow_id": workflow_id,
                "execution_mode": args.mode or "auto",
                "parallel_enabled": args.parallel,
                "max_workers": args.workers or 4,
                "timeout": args.timeout or 3600,
                "input_data": json.loads(args.input) if args.input else {}
            }
            
            print(f"ğŸš€ å¼€å§‹æ‰§è¡Œå·¥ä½œæµ: {workflow_id}")
            print(f"âš™ï¸ æ‰§è¡Œæ¨¡å¼: {execution_config['execution_mode']}")
            print(f"ğŸ‘¥ æœ€å¤§å·¥ä½œçº¿ç¨‹: {execution_config['max_workers']}")
            
            result = await self.scheduler.execute_workflow(execution_config)
            
            if result["status"] == "success":
                print(f"âœ… å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
                print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {result.get('execution_time', 'N/A')}ç§’")
                print(f"ğŸ“Š æˆåŠŸèŠ‚ç‚¹: {result.get('successful_nodes', 0)}")
                print(f"âŒ å¤±è´¥èŠ‚ç‚¹: {result.get('failed_nodes', 0)}")
                
                if args.output:
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    print(f"ğŸ’¾ æ‰§è¡Œç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            else:
                print(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {result['message']}")
                return 1
                
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå·¥ä½œæµæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return 1
        
        return 0
    
    async def analyze_dependencies(self, args):
        """åˆ†æä¾èµ–å…³ç³»"""
        try:
            # åŠ è½½å·¥ä½œæµ
            if args.file:
                with open(args.file, 'r', encoding='utf-8') as f:
                    workflow_data = json.load(f)
                workflow_id = workflow_data.get("workflow_id")
            else:
                workflow_id = args.id
            
            if not workflow_id:
                print("âŒ è¯·æä¾›å·¥ä½œæµIDæˆ–æ–‡ä»¶")
                return 1
            
            # è·å–å·¥ä½œæµ
            workflow = await self.state_manager.get_workflow(workflow_id)
            if not workflow:
                print(f"âŒ å·¥ä½œæµä¸å­˜åœ¨: {workflow_id}")
                return 1
            
            print(f"ğŸ” åˆ†æå·¥ä½œæµä¾èµ–å…³ç³»: {workflow_id}")
            
            result = await self.dependency_manager.analyze_dependencies(workflow)
            
            if result["status"] == "success":
                analysis = result["analysis"]
                print(f"âœ… ä¾èµ–åˆ†æå®Œæˆ")
                print(f"ğŸ“Š æ€»èŠ‚ç‚¹æ•°: {analysis['total_nodes']}")
                print(f"ğŸ”— æ€»ä¾èµ–æ•°: {analysis['total_dependencies']}")
                print(f"âš ï¸ æ£€æµ‹åˆ°å†²çª: {analysis['conflicts_detected']}")
                
                # æ˜¾ç¤ºæ‹“æ‰‘æ’åº
                if analysis["topological_order"]:
                    print(f"\nğŸ“‹ æ‹“æ‰‘æ’åº (åˆ†å±‚):")
                    for i, layer in enumerate(analysis["topological_order"]):
                        print(f"  å±‚ {i+1}: {', '.join(layer)}")
                
                # æ˜¾ç¤ºå…³é”®è·¯å¾„
                if analysis["critical_paths"]:
                    print(f"\nğŸ¯ å…³é”®è·¯å¾„:")
                    for i, path in enumerate(analysis["critical_paths"][:3]):
                        print(f"  è·¯å¾„ {i+1}: {' -> '.join(path['path'])} (æƒé‡: {path['weight']})")
                
                # æ˜¾ç¤ºå¹¶è¡Œæœºä¼š
                if analysis["parallel_opportunities"]:
                    print(f"\nâš¡ å¹¶è¡Œæ‰§è¡Œæœºä¼š:")
                    for opp in analysis["parallel_opportunities"]:
                        if opp.get("opportunity_type") == "topological_parallelism":
                            print(f"  å±‚ {opp['layer']}: {opp['parallel_count']} ä¸ªèŠ‚ç‚¹å¯å¹¶è¡Œ")
                
                # æ˜¾ç¤ºå†²çª
                if result["conflicts"]:
                    print(f"\nâš ï¸ æ£€æµ‹åˆ°çš„å†²çª:")
                    for conflict in result["conflicts"]:
                        print(f"  - {conflict['conflict_type']}: {conflict['description']}")
                
                # æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
                if analysis["optimization_suggestions"]:
                    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                    for suggestion in analysis["optimization_suggestions"]:
                        print(f"  - {suggestion}")
                
                if args.output:
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            else:
                print(f"âŒ ä¾èµ–åˆ†æå¤±è´¥: {result['message']}")
                return 1
                
        except Exception as e:
            print(f"âŒ åˆ†æä¾èµ–å…³ç³»æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return 1
        
        return 0
    
    async def list_workflows(self, args):
        """åˆ—å‡ºå·¥ä½œæµ"""
        try:
            result = await self.state_manager.list_workflows()
            
            if result["status"] == "success":
                workflows = result["workflows"]
                print(f"ğŸ“‹ å…±æ‰¾åˆ° {len(workflows)} ä¸ªå·¥ä½œæµ:")
                print()
                
                for workflow in workflows:
                    status_icon = "âœ…" if workflow["status"] == "completed" else "ğŸ”„" if workflow["status"] == "running" else "â¸ï¸"
                    print(f"{status_icon} {workflow['id']}")
                    print(f"   åç§°: {workflow['name']}")
                    print(f"   çŠ¶æ€: {workflow['status']}")
                    print(f"   åˆ›å»ºæ—¶é—´: {workflow['created_at']}")
                    print(f"   èŠ‚ç‚¹æ•°: {workflow.get('nodes_count', 'N/A')}")
                    print()
            else:
                print(f"âŒ è·å–å·¥ä½œæµåˆ—è¡¨å¤±è´¥: {result['message']}")
                return 1
                
        except Exception as e:
            print(f"âŒ åˆ—å‡ºå·¥ä½œæµæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return 1
        
        return 0
    
    async def status(self, args):
        """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
        try:
            print("ğŸ“Š å¢å¼ºå‹å·¥ä½œæµå¼•æ“çŠ¶æ€")
            print("=" * 50)
            
            # å·¥ä½œæµçŠ¶æ€
            workflow_status = await self.state_manager.get_system_status()
            print(f"ğŸ”§ å·¥ä½œæµç®¡ç†:")
            print(f"   æ€»å·¥ä½œæµæ•°: {workflow_status.get('total_workflows', 0)}")
            print(f"   è¿è¡Œä¸­: {workflow_status.get('running_workflows', 0)}")
            print(f"   å·²å®Œæˆ: {workflow_status.get('completed_workflows', 0)}")
            print(f"   å¤±è´¥: {workflow_status.get('failed_workflows', 0)}")
            
            # ä¾èµ–ç®¡ç†çŠ¶æ€
            dependency_status = self.dependency_manager.get_dependency_status()
            print(f"\nğŸ”— ä¾èµ–ç®¡ç†:")
            print(f"   æ€»èŠ‚ç‚¹æ•°: {dependency_status['total_nodes']}")
            print(f"   æ€»è¾¹æ•°: {dependency_status['total_edges']}")
            print(f"   å†²çªæ•°: {dependency_status['conflicts']['total']}")
            print(f"   èµ„æºå†²çª: {dependency_status['resources']['resource_conflicts']}")
            
            # æ‰§è¡Œè°ƒåº¦çŠ¶æ€
            scheduler_status = await self.scheduler.get_scheduler_status()
            print(f"\nâš¡ æ‰§è¡Œè°ƒåº¦:")
            print(f"   æ´»è·ƒæ‰§è¡Œå™¨: {scheduler_status.get('active_executors', 0)}")
            print(f"   é˜Ÿåˆ—ä¸­ä»»åŠ¡: {scheduler_status.get('queued_tasks', 0)}")
            print(f"   æ€»æ‰§è¡Œæ¬¡æ•°: {scheduler_status.get('total_executions', 0)}")
            print(f"   æˆåŠŸç‡: {scheduler_status.get('success_rate', 0):.1f}%")
            
            # ç”Ÿæˆå™¨çŠ¶æ€
            generator_status = await self.generator.get_generator_status()
            print(f"\nğŸ—ï¸ å·¥ä½œæµç”Ÿæˆ:")
            print(f"   å¯ç”¨æ¨¡æ¿: {generator_status.get('available_templates', 0)}")
            print(f"   ç”Ÿæˆæ¬¡æ•°: {generator_status.get('generation_count', 0)}")
            print(f"   ç¼“å­˜å‘½ä¸­ç‡: {generator_status.get('cache_hit_rate', 0):.1f}%")
            
        except Exception as e:
            print(f"âŒ è·å–ç³»ç»ŸçŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return 1
        
        return 0
    
    async def optimize_workflow(self, args):
        """ä¼˜åŒ–å·¥ä½œæµ"""
        try:
            # åŠ è½½å·¥ä½œæµ
            if args.file:
                with open(args.file, 'r', encoding='utf-8') as f:
                    workflow_data = json.load(f)
                workflow_id = workflow_data.get("workflow_id")
            else:
                workflow_id = args.id
            
            if not workflow_id:
                print("âŒ è¯·æä¾›å·¥ä½œæµIDæˆ–æ–‡ä»¶")
                return 1
            
            print(f"ğŸ”§ ä¼˜åŒ–å·¥ä½œæµ: {workflow_id}")
            
            optimization_config = {
                "workflow_id": workflow_id,
                "optimization_type": args.type or "auto",
                "enable_parallelization": args.parallel,
                "resolve_conflicts": args.resolve_conflicts,
                "optimize_resources": args.optimize_resources
            }
            
            result = await self.engine.optimize_workflow(optimization_config)
            
            if result["status"] == "success":
                print(f"âœ… å·¥ä½œæµä¼˜åŒ–å®Œæˆ")
                print(f"ğŸ“ˆ ä¼˜åŒ–æ”¹è¿›:")
                
                improvements = result.get("improvements", {})
                for metric, improvement in improvements.items():
                    print(f"   {metric}: {improvement}")
                
                if args.output:
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    print(f"ğŸ’¾ ä¼˜åŒ–ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            else:
                print(f"âŒ å·¥ä½œæµä¼˜åŒ–å¤±è´¥: {result['message']}")
                return 1
                
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–å·¥ä½œæµæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return 1
        
        return 0

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Enhanced Workflow MCP CLI")
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # åˆ›å»ºå·¥ä½œæµ
    create_parser = subparsers.add_parser("create", help="åˆ›å»ºå·¥ä½œæµ")
    create_parser.add_argument("name", help="å·¥ä½œæµåç§°")
    create_parser.add_argument("--description", help="å·¥ä½œæµæè¿°")
    create_parser.add_argument("--template", help="æ¨¡æ¿ç±»å‹ (basic/professional/composite)")
    create_parser.add_argument("--requirements", help="éœ€æ±‚åˆ—è¡¨ (é€—å·åˆ†éš”)")
    create_parser.add_argument("--parallel", action="store_true", help="å¯ç”¨å¹¶è¡Œæ‰§è¡Œ")
    create_parser.add_argument("--optimize", action="store_true", help="å¯ç”¨è‡ªåŠ¨ä¼˜åŒ–")
    create_parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    # æ‰§è¡Œå·¥ä½œæµ
    execute_parser = subparsers.add_parser("execute", help="æ‰§è¡Œå·¥ä½œæµ")
    execute_parser.add_argument("--id", help="å·¥ä½œæµID")
    execute_parser.add_argument("--file", help="å·¥ä½œæµæ–‡ä»¶è·¯å¾„")
    execute_parser.add_argument("--mode", help="æ‰§è¡Œæ¨¡å¼ (auto/manual/debug)")
    execute_parser.add_argument("--parallel", action="store_true", help="å¯ç”¨å¹¶è¡Œæ‰§è¡Œ")
    execute_parser.add_argument("--workers", type=int, help="æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°")
    execute_parser.add_argument("--timeout", type=int, help="è¶…æ—¶æ—¶é—´(ç§’)")
    execute_parser.add_argument("--input", help="è¾“å…¥æ•°æ® (JSONæ ¼å¼)")
    execute_parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    # åˆ†æä¾èµ–
    analyze_parser = subparsers.add_parser("analyze", help="åˆ†æä¾èµ–å…³ç³»")
    analyze_parser.add_argument("--id", help="å·¥ä½œæµID")
    analyze_parser.add_argument("--file", help="å·¥ä½œæµæ–‡ä»¶è·¯å¾„")
    analyze_parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    # åˆ—å‡ºå·¥ä½œæµ
    list_parser = subparsers.add_parser("list", help="åˆ—å‡ºå·¥ä½œæµ")
    
    # ç³»ç»ŸçŠ¶æ€
    status_parser = subparsers.add_parser("status", help="æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€")
    
    # ä¼˜åŒ–å·¥ä½œæµ
    optimize_parser = subparsers.add_parser("optimize", help="ä¼˜åŒ–å·¥ä½œæµ")
    optimize_parser.add_argument("--id", help="å·¥ä½œæµID")
    optimize_parser.add_argument("--file", help="å·¥ä½œæµæ–‡ä»¶è·¯å¾„")
    optimize_parser.add_argument("--type", help="ä¼˜åŒ–ç±»å‹ (auto/performance/resource)")
    optimize_parser.add_argument("--parallel", action="store_true", help="å¯ç”¨å¹¶è¡Œä¼˜åŒ–")
    optimize_parser.add_argument("--resolve-conflicts", action="store_true", help="è§£å†³å†²çª")
    optimize_parser.add_argument("--optimize-resources", action="store_true", help="ä¼˜åŒ–èµ„æºä½¿ç”¨")
    optimize_parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    # åˆ›å»ºCLIå®ä¾‹å¹¶æ‰§è¡Œå‘½ä»¤
    cli = EnhancedWorkflowCLI()
    
    try:
        if args.command == "create":
            return asyncio.run(cli.create_workflow(args))
        elif args.command == "execute":
            return asyncio.run(cli.execute_workflow(args))
        elif args.command == "analyze":
            return asyncio.run(cli.analyze_dependencies(args))
        elif args.command == "list":
            return asyncio.run(cli.list_workflows(args))
        elif args.command == "status":
            return asyncio.run(cli.status(args))
        elif args.command == "optimize":
            return asyncio.run(cli.optimize_workflow(args))
        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤: {args.command}")
            return 1
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æ“ä½œå·²å–æ¶ˆ")
        return 1
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå‘½ä»¤æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())

