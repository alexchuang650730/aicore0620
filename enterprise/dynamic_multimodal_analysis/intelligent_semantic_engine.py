# -*- coding: utf-8 -*-
"""
æ™ºèƒ½èªç¾©ç†è§£å¼•æ“
å»é™¤ç¡¬ç·¨ç¢¼ï¼Œä½¿ç”¨AIé€²è¡Œå‹•æ…‹å•é¡Œç†è§£å’Œå›ç­”ç”Ÿæˆ
"""

import re
import json
from typing import Dict, List, Any, Optional
import logging

logger = logging.getLogger(__name__)

class IntelligentSemanticEngine:
    """æ™ºèƒ½èªç¾©ç†è§£å¼•æ“"""
    
    def __init__(self, ai_client=None):
        self.ai_client = ai_client
        self.domain_knowledge = self._load_domain_knowledge()
        
    def _load_domain_knowledge(self) -> Dict[str, Any]:
        """åŠ è¼‰é ˜åŸŸçŸ¥è­˜åº«"""
        return {
            "insurance_metrics": [
                "è‡ªå‹•åŒ–æ¯”ç‡", "è™•ç†æ™‚é–“", "äººåŠ›é…ç½®", "æº–ç¢ºç‡", 
                "è™•ç†é‡", "æˆæœ¬æ•ˆç›Š", "æŠ•è³‡å›å ±", "é¢¨éšªè©•ä¼°"
            ],
            "process_types": [
                "æ ¸ä¿æµç¨‹", "ç†è³ æµç¨‹", "æ‰¿ä¿æµç¨‹", "å‡ºå–®æµç¨‹",
                "æ”¶æ–‡æµç¨‹", "å¯©æ ¸æµç¨‹", "å»ºæª”æµç¨‹"
            ],
            "measurement_units": [
                "äººæœˆ", "å·¥æ™‚", "ä»¶æ•¸", "æ¯”ç‡", "é‡‘é¡", "æ™‚é–“"
            ]
        }
    
    def understand_user_intent(self, user_requirement: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIç†è§£ç”¨æˆ¶çœŸæ­£çš„å•é¡Œæ„åœ–"""
        
        if not self.ai_client:
            # å¦‚æœæ²’æœ‰AIå®¢æˆ¶ç«¯ï¼Œä½¿ç”¨åŸºç¤èªç¾©åˆ†æ
            return self._basic_semantic_analysis(user_requirement)
        
        try:
            # æ§‹å»ºæ™ºèƒ½åˆ†ææç¤º
            analysis_prompt = self._build_intent_analysis_prompt(user_requirement)
            
            # èª¿ç”¨AIé€²è¡Œèªç¾©ç†è§£
            ai_response = self.ai_client.generate_response(analysis_prompt)
            
            # è§£æAIå›æ‡‰
            intent_analysis = self._parse_intent_response(ai_response)
            
            logger.info(f"AIèªç¾©ç†è§£çµæœ: {intent_analysis}")
            return intent_analysis
            
        except Exception as e:
            logger.warning(f"AIèªç¾©ç†è§£å¤±æ•—ï¼Œä½¿ç”¨åŸºç¤åˆ†æ: {e}")
            return self._basic_semantic_analysis(user_requirement)
    
    def _build_intent_analysis_prompt(self, user_requirement: str) -> str:
        """æ§‹å»ºæ„åœ–åˆ†ææç¤º"""
        return f"""
è«‹åˆ†æä»¥ä¸‹ç”¨æˆ¶éœ€æ±‚ï¼Œæå–æ ¸å¿ƒå•é¡Œå’ŒæœŸæœ›çš„ä¿¡æ¯é¡å‹ï¼š

ç”¨æˆ¶éœ€æ±‚: "{user_requirement}"

è«‹å¾ä»¥ä¸‹è§’åº¦åˆ†æï¼š
1. ç”¨æˆ¶æƒ³äº†è§£çš„å…·é«”æŒ‡æ¨™æˆ–æ•¸æ“š
2. ç”¨æˆ¶é—œå¿ƒçš„æ¥­å‹™æµç¨‹æˆ–ç’°ç¯€  
3. ç”¨æˆ¶æœŸæœ›çš„é‡åŒ–ä¿¡æ¯é¡å‹
4. å•é¡Œçš„å„ªå…ˆç´šå’Œé‡è¦ç¨‹åº¦

è«‹ä»¥JSONæ ¼å¼è¿”å›åˆ†æçµæœï¼š
{{
    "core_questions": ["å•é¡Œ1", "å•é¡Œ2", ...],
    "target_metrics": ["æŒ‡æ¨™1", "æŒ‡æ¨™2", ...],
    "business_processes": ["æµç¨‹1", "æµç¨‹2", ...],
    "expected_data_types": ["æ•¸æ“šé¡å‹1", "æ•¸æ“šé¡å‹2", ...],
    "priority_level": "high/medium/low",
    "answer_strategy": "å¦‚ä½•çµ„ç¹”å›ç­”çš„ç­–ç•¥"
}}
"""
    
    def _parse_intent_response(self, ai_response: str) -> Dict[str, Any]:
        """è§£æAIçš„æ„åœ–åˆ†æå›æ‡‰"""
        try:
            # å˜—è©¦æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', ai_response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                # å¦‚æœæ²’æœ‰JSONï¼Œé€²è¡Œæ–‡æœ¬è§£æ
                return self._parse_text_response(ai_response)
        except Exception as e:
            logger.warning(f"è§£æAIå›æ‡‰å¤±æ•—: {e}")
            return self._basic_semantic_analysis(ai_response)
    
    def _basic_semantic_analysis(self, text: str) -> Dict[str, Any]:
        """åŸºç¤èªç¾©åˆ†æï¼ˆç„¡AIæ™‚çš„å‚™ç”¨æ–¹æ¡ˆï¼‰"""
        
        # å‹•æ…‹æå–é—œéµæ¦‚å¿µ
        metrics = self._extract_metrics_concepts(text)
        processes = self._extract_process_concepts(text)
        data_types = self._extract_data_type_concepts(text)
        
        # åˆ†æå•é¡Œé¡å‹
        question_types = self._analyze_question_types(text)
        
        return {
            "core_questions": question_types,
            "target_metrics": metrics,
            "business_processes": processes,
            "expected_data_types": data_types,
            "priority_level": self._assess_priority(text),
            "answer_strategy": self._determine_answer_strategy(question_types, metrics)
        }
    
    def _extract_metrics_concepts(self, text: str) -> List[str]:
        """å‹•æ…‹æå–æŒ‡æ¨™æ¦‚å¿µ"""
        found_metrics = []
        
        # ä½¿ç”¨èªç¾©ç›¸ä¼¼åº¦è€Œä¸æ˜¯ç¡¬ç·¨ç¢¼åŒ¹é…
        for metric in self.domain_knowledge["insurance_metrics"]:
            if self._semantic_similarity(text, metric) > 0.3:
                found_metrics.append(metric)
        
        # æå–æ•¸å­—ç›¸é—œçš„æ¦‚å¿µ
        number_contexts = re.findall(r'(\w+[^ã€‚ï¼Œ,.\n]*\d+[%\w]*[^ã€‚ï¼Œ,.\n]*)', text)
        for context in number_contexts:
            metric_type = self._classify_metric_from_context(context)
            if metric_type and metric_type not in found_metrics:
                found_metrics.append(metric_type)
        
        return found_metrics
    
    def _extract_process_concepts(self, text: str) -> List[str]:
        """å‹•æ…‹æå–æµç¨‹æ¦‚å¿µ"""
        found_processes = []
        
        # è­˜åˆ¥æµç¨‹ç›¸é—œè©å½™
        process_indicators = ['æµç¨‹', 'ç¨‹åº', 'ä½œæ¥­', 'SOP', 'æ¨™æº–', 'æ­¥é©Ÿ']
        
        for indicator in process_indicators:
            if indicator in text:
                # æå–è©²æŒ‡ç¤ºè©å‘¨åœçš„ä¸Šä¸‹æ–‡
                contexts = self._extract_context_around_word(text, indicator, window=10)
                for context in contexts:
                    process_type = self._classify_process_from_context(context)
                    if process_type and process_type not in found_processes:
                        found_processes.append(process_type)
        
        return found_processes
    
    def _extract_data_type_concepts(self, text: str) -> List[str]:
        """å‹•æ…‹æå–æ•¸æ“šé¡å‹æ¦‚å¿µ"""
        data_types = []
        
        # è­˜åˆ¥é‡åŒ–è©å½™
        if re.search(r'å¤šå°‘|å¹¾å€‹|æ•¸é‡|æ¯”ç‡|ç™¾åˆ†æ¯”', text):
            data_types.append("æ•¸é‡çµ±è¨ˆ")
        
        if re.search(r'æ™‚é–“|æœŸé–“|é€±æœŸ|æœˆ|å¹´', text):
            data_types.append("æ™‚é–“åˆ†æ")
        
        if re.search(r'äºº|å“¡å·¥|äººåŠ›|åœ˜éšŠ', text):
            data_types.append("äººåŠ›è³‡æº")
        
        if re.search(r'æˆæœ¬|è²»ç”¨|æŠ•è³‡|æ•ˆç›Š', text):
            data_types.append("æˆæœ¬æ•ˆç›Š")
        
        return data_types
    
    def _semantic_similarity(self, text1: str, text2: str) -> float:
        """è¨ˆç®—èªç¾©ç›¸ä¼¼åº¦ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        # é€™è£¡å¯ä»¥ä½¿ç”¨æ›´è¤‡é›œçš„èªç¾©ç›¸ä¼¼åº¦ç®—æ³•
        # ç›®å‰ä½¿ç”¨ç°¡å–®çš„è©å½™é‡ç–Šåº¦
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def _classify_metric_from_context(self, context: str) -> Optional[str]:
        """å¾ä¸Šä¸‹æ–‡åˆ†é¡æŒ‡æ¨™é¡å‹"""
        context_lower = context.lower()
        
        if any(word in context_lower for word in ['è‡ªå‹•', 'è‡ªå‹•åŒ–', 'æ©Ÿå™¨']):
            return "è‡ªå‹•åŒ–æŒ‡æ¨™"
        elif any(word in context_lower for word in ['æº–ç¢º', 'ç²¾ç¢º', 'æ­£ç¢º']):
            return "æº–ç¢ºæ€§æŒ‡æ¨™"
        elif any(word in context_lower for word in ['æ™‚é–“', 'é€Ÿåº¦', 'æ•ˆç‡']):
            return "æ•ˆç‡æŒ‡æ¨™"
        elif any(word in context_lower for word in ['äºº', 'å“¡å·¥', 'äººåŠ›']):
            return "äººåŠ›æŒ‡æ¨™"
        elif any(word in context_lower for word in ['æˆæœ¬', 'è²»ç”¨', 'æŠ•è³‡']):
            return "æˆæœ¬æŒ‡æ¨™"
        
        return None
    
    def _classify_process_from_context(self, context: str) -> Optional[str]:
        """å¾ä¸Šä¸‹æ–‡åˆ†é¡æµç¨‹é¡å‹"""
        context_lower = context.lower()
        
        if any(word in context_lower for word in ['æ ¸ä¿', 'å¯©æ ¸', 'è©•ä¼°']):
            return "æ ¸ä¿æµç¨‹"
        elif any(word in context_lower for word in ['æ”¶æ–‡', 'é€²ä»¶', 'æ¥æ”¶']):
            return "æ”¶æ–‡æµç¨‹"
        elif any(word in context_lower for word in ['å‡ºå–®', 'å»ºæª”', 'ç™¼è¡Œ']):
            return "å‡ºå–®æµç¨‹"
        elif any(word in context_lower for word in ['ç†è³ ', 'è³ ä»˜', 'çµ¦ä»˜']):
            return "ç†è³ æµç¨‹"
        
        return None
    
    def _extract_context_around_word(self, text: str, word: str, window: int = 5) -> List[str]:
        """æå–è©å½™å‘¨åœçš„ä¸Šä¸‹æ–‡"""
        contexts = []
        words = text.split()
        
        for i, w in enumerate(words):
            if word in w:
                start = max(0, i - window)
                end = min(len(words), i + window + 1)
                context = ' '.join(words[start:end])
                contexts.append(context)
        
        return contexts
    
    def _analyze_question_types(self, text: str) -> List[str]:
        """åˆ†æå•é¡Œé¡å‹"""
        questions = []
        
        # è­˜åˆ¥ç–‘å•è©å’Œå•é¡Œæ¨¡å¼
        if re.search(r'å¤šå°‘|å¹¾å€‹|ä»€éº¼|å¦‚ä½•|æ€éº¼', text):
            if re.search(r'å¤šå°‘.*äºº|å¹¾å€‹.*äºº|äººåŠ›|äººå“¡', text):
                questions.append("äººåŠ›éœ€æ±‚åˆ†æ")
            
            if re.search(r'æ¯”ç‡|ç™¾åˆ†æ¯”|è‡ªå‹•åŒ–', text):
                questions.append("è‡ªå‹•åŒ–ç¨‹åº¦åˆ†æ")
            
            if re.search(r'æ™‚é–“|æœŸé–“|é€±æœŸ', text):
                questions.append("æ™‚é–“æ•ˆç‡åˆ†æ")
            
            if re.search(r'æµç¨‹|ç¨‹åº|æ­¥é©Ÿ', text):
                questions.append("æµç¨‹åˆ†æ")
        
        return questions if questions else ["ç¶œåˆæ¥­å‹™åˆ†æ"]
    
    def _assess_priority(self, text: str) -> str:
        """è©•ä¼°å•é¡Œå„ªå…ˆç´š"""
        high_priority_indicators = ['æ€¥', 'é‡è¦', 'é—œéµ', 'æ ¸å¿ƒ', 'ä¸»è¦']
        medium_priority_indicators = ['ä¸€èˆ¬', 'æ™®é€š', 'å¸¸è¦']
        
        if any(indicator in text for indicator in high_priority_indicators):
            return "high"
        elif any(indicator in text for indicator in medium_priority_indicators):
            return "medium"
        else:
            return "medium"  # é»˜èªä¸­ç­‰å„ªå…ˆç´š
    
    def _determine_answer_strategy(self, question_types: List[str], metrics: List[str]) -> str:
        """ç¢ºå®šå›ç­”ç­–ç•¥"""
        if "äººåŠ›éœ€æ±‚åˆ†æ" in question_types:
            return "å„ªå…ˆæä¾›äººåŠ›é…ç½®å’Œå·¥ä½œé‡åˆ†æ"
        elif "è‡ªå‹•åŒ–ç¨‹åº¦åˆ†æ" in question_types:
            return "é‡é»åˆ†æè‡ªå‹•åŒ–ç¾ç‹€å’Œè¡Œæ¥­å°æ¯”"
        elif len(metrics) > 3:
            return "æä¾›å…¨é¢çš„é‡åŒ–åˆ†æå ±å‘Š"
        else:
            return "æä¾›é‡å°æ€§çš„å°ˆæ¥­åˆ†æ"

    def generate_intelligent_insights(self, intent_analysis: Dict[str, Any], 
                                    analysis_data: Dict[str, Any]) -> List[str]:
        """åŸºæ–¼æ„åœ–åˆ†æç”Ÿæˆæ™ºèƒ½æ´å¯Ÿ"""
        
        if not self.ai_client:
            return self._generate_rule_based_insights(intent_analysis, analysis_data)
        
        try:
            # æ§‹å»ºæ´å¯Ÿç”Ÿæˆæç¤º
            insight_prompt = self._build_insight_generation_prompt(intent_analysis, analysis_data)
            
            # èª¿ç”¨AIç”Ÿæˆæ´å¯Ÿ
            ai_response = self.ai_client.generate_response(insight_prompt)
            
            # è§£æç”Ÿæˆçš„æ´å¯Ÿ
            insights = self._parse_insights_response(ai_response)
            
            return insights
            
        except Exception as e:
            logger.warning(f"AIæ´å¯Ÿç”Ÿæˆå¤±æ•—ï¼Œä½¿ç”¨è¦å‰‡ç”Ÿæˆ: {e}")
            return self._generate_rule_based_insights(intent_analysis, analysis_data)
    
    def _build_insight_generation_prompt(self, intent_analysis: Dict[str, Any], 
                                       analysis_data: Dict[str, Any]) -> str:
        """æ§‹å»ºæ´å¯Ÿç”Ÿæˆæç¤º"""
        return f"""
åŸºæ–¼ä»¥ä¸‹ç”¨æˆ¶æ„åœ–åˆ†æå’Œæ•¸æ“šï¼Œç”Ÿæˆå°ˆæ¥­çš„æ¥­å‹™æ´å¯Ÿï¼š

ç”¨æˆ¶æ„åœ–åˆ†æï¼š
{json.dumps(intent_analysis, ensure_ascii=False, indent=2)}

åˆ†ææ•¸æ“šï¼š
{json.dumps(analysis_data, ensure_ascii=False, indent=2)}

è«‹ç”Ÿæˆ3-5å€‹å°ˆæ¥­æ´å¯Ÿï¼Œè¦æ±‚ï¼š
1. ç›´æ¥å›ç­”ç”¨æˆ¶çš„æ ¸å¿ƒå•é¡Œ
2. æä¾›å…·é«”çš„é‡åŒ–æ•¸æ“š
3. åŒ…å«è¡Œæ¥­å°æ¯”å’Œå»ºè­°
4. ä½¿ç”¨å°ˆæ¥­ä½†æ˜“æ‡‚çš„èªè¨€

è«‹ä»¥åˆ—è¡¨æ ¼å¼è¿”å›æ´å¯Ÿï¼Œæ¯å€‹æ´å¯Ÿä¸€è¡Œã€‚
"""
    
    def _parse_insights_response(self, ai_response: str) -> List[str]:
        """è§£æAIç”Ÿæˆçš„æ´å¯Ÿå›æ‡‰"""
        insights = []
        
        # æŒ‰è¡Œåˆ†å‰²ä¸¦æ¸…ç†
        lines = ai_response.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#') and len(line) > 10:
                # ç§»é™¤åˆ—è¡¨æ¨™è¨˜
                line = re.sub(r'^\d+\.\s*', '', line)
                line = re.sub(r'^[-*]\s*', '', line)
                insights.append(line)
        
        return insights[:5]  # æœ€å¤šè¿”å›5å€‹æ´å¯Ÿ
    
    def _generate_rule_based_insights(self, intent_analysis: Dict[str, Any], 
                                    analysis_data: Dict[str, Any]) -> List[str]:
        """åŸºæ–¼è¦å‰‡ç”Ÿæˆæ´å¯Ÿï¼ˆAIä¸å¯ç”¨æ™‚çš„å‚™ç”¨æ–¹æ¡ˆï¼‰"""
        insights = []
        
        # æ ¹æ“šå•é¡Œé¡å‹ç”Ÿæˆç›¸æ‡‰æ´å¯Ÿ
        for question_type in intent_analysis.get("core_questions", []):
            if "äººåŠ›éœ€æ±‚" in question_type:
                insights.append("ğŸ‘¥ äººåŠ›é…ç½®åˆ†æï¼šæ ¹æ“šæ¥­å‹™é‡å’Œæµç¨‹è¤‡é›œåº¦ï¼Œå»ºè­°é…ç½®å°ˆæ¥­åœ˜éšŠä»¥ç¢ºä¿è™•ç†æ•ˆç‡")
            
            elif "è‡ªå‹•åŒ–ç¨‹åº¦" in question_type:
                insights.append("ğŸ¤– è‡ªå‹•åŒ–ç¾ç‹€ï¼šç•¶å‰è‡ªå‹•åŒ–æ°´å¹³èˆ‡è¡Œæ¥­é ˜å…ˆè€…å­˜åœ¨å·®è·ï¼Œæœ‰è¼ƒå¤§æå‡ç©ºé–“")
            
            elif "æ™‚é–“æ•ˆç‡" in question_type:
                insights.append("â±ï¸ æ•ˆç‡åˆ†æï¼šè™•ç†æ™‚é–“å› æ¡ˆä»¶è¤‡é›œåº¦è€Œç•°ï¼Œæ¨™æº–åŒ–æµç¨‹å¯é¡¯è‘—æå‡æ•ˆç‡")
        
        # æ ¹æ“šæŒ‡æ¨™é¡å‹æ·»åŠ æ´å¯Ÿ
        for metric in intent_analysis.get("target_metrics", []):
            if "è‡ªå‹•åŒ–" in metric:
                insights.append("ğŸ“Š è‡ªå‹•åŒ–å»ºè­°ï¼šé€šéæŠ€è¡“å‡ç´šå’Œæµç¨‹å„ªåŒ–ï¼Œå¯å¯¦ç¾æ›´é«˜çš„è‡ªå‹•åŒ–æ¯”ç‡")
        
        return insights[:3] if insights else ["ğŸ“‹ ç¶œåˆåˆ†æï¼šåŸºæ–¼ç¾æœ‰æ•¸æ“šæä¾›å°ˆæ¥­çš„æ¥­å‹™åˆ†æå’Œæ”¹é€²å»ºè­°"]

